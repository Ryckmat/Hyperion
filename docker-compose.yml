# ============================================================================
# Docker Compose pour Hyperion v3.0 - Stack complète
# ============================================================================

version: '3.8'

services:
  # ========================================
  # SERVICES CORE (obligatoires)
  # ========================================

  # Vector Database - Stockage embeddings RAG
  qdrant:
    image: qdrant/qdrant:latest
    container_name: hyperion-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - hyperion-network

  # LLM Server - Ollama pour inference
  ollama:
    image: ollama/ollama:latest
    container_name: hyperion-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - hyperion-network
    # Support GPU optionnel
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # API Hyperion - Service principal
  hyperion-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: hyperion-api
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./modeles:/app/modeles
      - ./mlruns:/app/mlruns
      - ./logs:/app/logs
      # Mount du dossier de développement pour les repo à analyser
      - /home/kortazo/Documents:/mnt/repositories:ro
    environment:
      # Configuration des services Docker
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION=hyperion_repos
      - OLLAMA_BASE_URL=http://ollama:11434
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=hyperion123
      - NEO4J_DATABASE=neo4j

      # Configuration ML
      - EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
      - EMBEDDING_DEVICE=cpu  # Changez en 'cuda' si GPU disponible
      - EMBEDDING_DIM=1024

      # Configuration LLM
      - OLLAMA_MODEL=llama3.2:1b
      - LLM_TEMPERATURE=0.0
      - LLM_MAX_TOKENS=128
      - LLM_TIMEOUT=10
      - LLM_TOP_K=5

      # Configuration chunking
      - CHUNK_SIZE=512
      - CHUNK_OVERLAP=50

      # Performance
      - BATCH_SIZE_COMMITS=500
      - BATCH_SIZE_FILES=2000

      # Git
      - TAGS_REGEX=^v?\d+\.\d+\.\d+$
      - MAIN_CANDIDATES=main,master,trunk,develop

    depends_on:
      qdrant:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - hyperion-network

  # ========================================
  # SERVICES OPTIONNELS
  # ========================================

  # Graph Database - Neo4j pour relations de code
  neo4j:
    image: neo4j:5
    container_name: hyperion-neo4j
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_conf:/conf
    environment:
      - NEO4J_AUTH=neo4j/hyperion123
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_dbms_memory_pagecache_size=512m
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7474/"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - hyperion-network
    profiles:
      - full  # Activé uniquement avec --profile full

  # Dashboard Frontend - React
  hyperion-dashboard:
    image: node:18-alpine
    container_name: hyperion-dashboard
    working_dir: /app
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
    command: sh -c "npm install && npm start"
    environment:
      - REACT_APP_API_URL=http://localhost:8000
    depends_on:
      - hyperion-api
    restart: unless-stopped
    networks:
      - hyperion-network
    profiles:
      - full  # Activé uniquement avec --profile full

  # Chat Interface - Open WebUI
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: hyperion-openwebui
    ports:
      - "3001:8080"
    volumes:
      - openwebui_data:/app/backend/data
    environment:
      - WEBUI_AUTH=false
      - OLLAMA_BASE_URL=http://ollama:11434
      - OPENAI_API_BASE_URL=http://hyperion-api:8000/v1
      - OPENAI_API_KEY=sk-hyperion-dummy-key
      - DEFAULT_MODEL=hyperion-rag
    depends_on:
      - hyperion-api
      - ollama
    restart: unless-stopped
    networks:
      - hyperion-network
    profiles:
      - full  # Activé uniquement avec --profile full

# ========================================
# VOLUMES PERSISTANTS
# ========================================
volumes:
  # Core services
  qdrant_storage:
    driver: local
  ollama_models:
    driver: local

  # Optional services
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_conf:
    driver: local
  openwebui_data:
    driver: local

# ========================================
# RÉSEAU
# ========================================
networks:
  hyperion-network:
    driver: bridge