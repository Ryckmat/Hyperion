"""Query engine pour RAG avec validation qualit√© int√©gr√©e."""

import os
import time

from langchain_ollama import OllamaLLM
from qdrant_client import QdrantClient
from qdrant_client.models import FieldCondition, Filter, MatchValue
from sentence_transformers import SentenceTransformer

from hyperion.modules.rag.config import (
    EMBEDDING_DEVICE,
    EMBEDDING_MODEL,
    LLM_MAX_TOKENS,
    LLM_TEMPERATURE,
    LLM_TIMEOUT,
    LLM_TOP_K,
    OLLAMA_BASE_URL,
    OLLAMA_MODEL,
    QDRANT_COLLECTION,
    QDRANT_HOST,
    QDRANT_PORT,
    QUERY_PROMPT_TEMPLATE,
)

# Import du syst√®me de validation qualit√© v2.8
try:
    from hyperion.modules.rag.quality.response_validator import ResponseValidator

    VALIDATION_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è Validation qualit√© non disponible: {e}")
    VALIDATION_AVAILABLE = False


class RAGQueryEngine:
    """
    Moteur de requ√™tes RAG.

    Process :
    1. Question ‚Üí embedding
    2. Recherche top-k similaires dans Qdrant
    3. Assembly contexte
    4. Prompt au LLM Ollama
    5. Retour r√©ponse format√©e
    """

    def __init__(
        self,
        qdrant_host: str = QDRANT_HOST,
        qdrant_port: int = QDRANT_PORT,
        collection_name: str = QDRANT_COLLECTION,
        ollama_base_url: str = OLLAMA_BASE_URL,
        ollama_model: str = OLLAMA_MODEL,
    ):
        """Initialise le query engine avec validation qualit√© v2.8."""
        # Qdrant client
        self.qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
        self.collection_name = collection_name

        # Mod√®le embeddings avec fallback automatique
        print("üì• Chargement mod√®le embeddings...")
        try:
            self.embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=EMBEDDING_DEVICE)
            print(f"‚úÖ Embeddings pr√™ts ({EMBEDDING_DEVICE})")
        except Exception as e:
            if EMBEDDING_DEVICE == "cuda":
                print(f"‚ö†Ô∏è Erreur GPU embeddings: {e}")
                print("üîÑ Fallback automatique vers CPU...")
                self.embedding_model = SentenceTransformer(EMBEDDING_MODEL, device="cpu")
                print("‚úÖ Embeddings pr√™ts (cpu - fallback)")
            else:
                raise

        # LLM Ollama (optimis√©)
        print(f"ü§ñ Connexion √† Ollama ({ollama_model})...")
        self.llm = OllamaLLM(
            base_url=ollama_base_url,
            model=ollama_model,
            temperature=LLM_TEMPERATURE,
            num_predict=LLM_MAX_TOKENS,
            timeout=LLM_TIMEOUT,  # Timeout pour √©viter attentes longues
        )
        print("‚úÖ LLM pr√™t")

        # Syst√®me de validation qualit√© v2.8
        self.enable_validation = os.getenv("ENABLE_RESPONSE_VALIDATION", "true").lower() == "true"
        self.validation_mode = os.getenv("VALIDATION_MODE", "flag")  # "flag" ou "reject"

        if self.enable_validation and VALIDATION_AVAILABLE:
            print("üîç Initialisation validation qualit√© v2.8...")
            try:
                self.response_validator = ResponseValidator(self.embedding_model)
                print("‚úÖ Validation qualit√© pr√™te")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur init validation: {e}")
                self.enable_validation = False
                self.response_validator = None
        else:
            if not VALIDATION_AVAILABLE:
                print("‚ö†Ô∏è Module validation qualit√© non disponible")
            else:
                print("‚ÑπÔ∏è Validation qualit√© d√©sactiv√©e (ENABLE_RESPONSE_VALIDATION=false)")
            self.response_validator = None

    def query(self, question: str, repo_filter: str | None = None, top_k: int = LLM_TOP_K) -> dict:
        """
        R√©pond √† une question via RAG avec validation qualit√© v2.8.

        Args:
            question: Question en langage naturel
            repo_filter: Filtrer sur un repo sp√©cifique (optionnel)
            top_k: Nombre de chunks √† r√©cup√©rer

        Returns:
            {
                "answer": str,
                "sources": List[Dict],
                "question": str,
                "processing_time": float,
                "quality": Dict (si validation activ√©e)
            }
        """
        start_time = time.time()

        try:
            # 1. G√©n√©rer embedding de la question (optimis√©)
            question_embedding = self.embedding_model.encode(
                question,
                convert_to_numpy=True,
                show_progress_bar=False,  # D√©sactiver pour vitesse
            )

            # 2. Recherche dans Qdrant (ultra-optimis√©)
            search_filter = None
            if repo_filter:
                search_filter = Filter(
                    must=[FieldCondition(key="repo", match=MatchValue(value=repo_filter))]
                )

            search_results = self.qdrant_client.query_points(
                collection_name=self.collection_name,
                query=question_embedding.tolist(),
                limit=min(top_k, 1),  # Un seul chunk pour vitesse maximum
                query_filter=search_filter,
                timeout=1,  # Timeout Qdrant strict
            ).points

            # 3. Assembler contexte (optimis√©, texte r√©duit)
            context_parts = []
            full_context_chunks = []  # Chunks complets pour validation qualit√©
            sources = []
            source_scores = []

            for result in search_results:
                payload = result.payload

                # Texte complet pour validation qualit√©
                full_text = payload["text"]
                full_context_chunks.append(full_text)
                source_scores.append(result.score)

                # Ultra-limitation pour vitesse maximale (comme avant)
                display_text = full_text
                if len(display_text) > 100:  # Extr√™me r√©duction 200‚Üí100
                    display_text = display_text[:100] + "..."

                context_parts.append(display_text)
                sources.append(
                    {
                        "repo": payload["repo"],
                        "section": payload["section"],
                        "score": result.score,
                        "text": display_text[:30] + "...",  # Preview ultra r√©duit 50‚Üí30
                    }
                )

            context = "\n\n---\n\n".join(context_parts)

            # 4. Construire prompt (optimis√©)
            full_prompt = QUERY_PROMPT_TEMPLATE.format(context=context, question=question)

            # 5. Appeler LLM avec timeout
            answer = self.llm.invoke(full_prompt).strip()

            processing_time = time.time() - start_time

            # 6. NOUVEAU: Validation qualit√© v2.8
            validation_result = None
            if self.enable_validation and self.response_validator:
                try:
                    validation_result = self.response_validator.validate_response(
                        answer=answer,
                        question=question,
                        context_chunks=full_context_chunks,
                        source_scores=source_scores,
                        processing_time=processing_time,
                    )

                    # Traitement selon mode validation
                    if validation_result["action"] == "reject" and self.validation_mode == "reject":
                        original_answer = answer
                        answer = "Je ne peux pas r√©pondre avec certitude bas√©e sur les sources disponibles. Veuillez reformuler votre question ou √™tre plus sp√©cifique."
                        validation_result["original_answer"] = original_answer
                        validation_result["answer_modified"] = True

                except Exception as validation_error:
                    print(f"‚ö†Ô∏è Erreur validation qualit√©: {validation_error}")
                    # Continue sans validation en cas d'erreur

            # 7. Construire r√©ponse finale
            response = {
                "answer": answer,
                "sources": sources,
                "question": question,
                "repo_filter": repo_filter,
                "processing_time": round(processing_time, 2),
                "performance": "fast" if processing_time < 3.0 else "slow",
            }

            # Ajouter m√©tadonn√©es qualit√© si validation activ√©e
            if validation_result:
                response["quality"] = {
                    "confidence": validation_result["confidence"],
                    "grade": validation_result["quality_grade"],
                    "action": validation_result["action"],
                    "should_flag": validation_result["should_flag"],
                    "hallucination_detected": validation_result["hallucination_analysis"][
                        "is_hallucination"
                    ],
                    "hallucination_severity": validation_result["hallucination_analysis"][
                        "severity"
                    ],
                    "recommendations": validation_result["recommendations"][:3],  # Limiter pour API
                    "validation_time": validation_result["validation_metadata"]["validation_time"],
                    "answer_modified": validation_result.get("answer_modified", False),
                }

            return response

        except Exception as e:
            processing_time = time.time() - start_time
            return {
                "answer": f"Erreur lors du traitement de la question: {str(e)}",
                "sources": [],
                "question": question,
                "repo_filter": repo_filter,
                "processing_time": round(processing_time, 2),
                "performance": "error",
            }

    def chat(
        self, question: str, repo: str | None = None, history: list[dict] | None = None
    ) -> dict:
        """
        Chat avec historique de conversation.

        Args:
            question: Question actuelle
            repo: Repo √† filtrer (optionnel)
            history: Historique [{role: "user/assistant", content: "..."}]

        Returns:
            M√™me format que query() avec historique mis √† jour
        """
        # TODO: Int√©grer l'historique dans le prompt
        # Pour l'instant, simple query
        result = self.query(question, repo_filter=repo)

        # Ajouter √† l'historique
        if history is None:
            history = []

        history.append({"role": "user", "content": question})
        history.append({"role": "assistant", "content": result["answer"]})

        result["history"] = history

        return result
