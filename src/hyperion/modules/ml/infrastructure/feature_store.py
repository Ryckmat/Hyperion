"""
Feature Store pour gestion centralis√©e des features ML.

Stockage, versioning et r√©cup√©ration des features calcul√©es pour:
- √âviter recalculs co√ªteux
- Garantir coh√©rence des features entre entra√Ænement/pr√©diction
- Historique et versioning des features
- Partage entre diff√©rents mod√®les
"""

import hashlib
import json
import pickle
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

from pydantic import BaseModel

from .ml_config import ml_config


class FeatureMetadata(BaseModel):
    """M√©tadonn√©es d'un ensemble de features."""

    feature_set_id: str
    source_file: str
    repository: str
    extracted_at: datetime
    feature_names: list[str]
    n_features: int
    extraction_version: str = "3.0.0"

    # Hash pour d√©tection de changements
    content_hash: str
    source_hash: str  # Hash du fichier source

    # M√©tadonn√©es suppl√©mentaires
    tags: dict[str, str] = {}
    extraction_time_ms: float | None = None

    class Config:
        arbitrary_types_allowed = True


class FeatureStore:
    """
    Feature Store professionnel pour gestion des features ML.

    Fonctionnalit√©s:
    - Stockage features par fichier/repository
    - Versioning et invalidation automatique
    - Cache avec TTL configurable
    - Recherche et filtrage
    - M√©tadonn√©es compl√®tes
    """

    def __init__(self, ttl_hours: int = 24):
        """
        Initialise le feature store.

        Args:
            ttl_hours: Dur√©e de vie du cache en heures
        """
        self.store_dir = ml_config.data_dir / "feature_store"
        self.metadata_dir = self.store_dir / "metadata"
        self.cache_dir = self.store_dir / "cache"

        # Cr√©er dossiers
        for directory in [self.store_dir, self.metadata_dir, self.cache_dir]:
            directory.mkdir(parents=True, exist_ok=True)

        self.ttl = timedelta(hours=ttl_hours)

    def store_features(
        self,
        features: dict[str, Any],
        source_file: str,
        repository: str,
        feature_names: list[str] | None = None,
        tags: dict[str, str] | None = None,
        extraction_time_ms: float | None = None,
    ) -> str:
        """
        Stocke un ensemble de features avec m√©tadonn√©es.

        Args:
            features: Dictionnaire des features calcul√©es
            source_file: Chemin du fichier source
            repository: Nom du repository
            feature_names: Liste explicite des noms de features
            tags: Tags additionnels
            extraction_time_ms: Temps d'extraction en millisecondes

        Returns:
            ID unique du feature set stock√©
        """
        # G√©n√©rer ID unique bas√© sur fichier + repository
        feature_set_id = self._generate_feature_set_id(source_file, repository)

        # Calculer hash du contenu
        content_hash = self._calculate_content_hash(features)
        source_hash = (
            self._calculate_file_hash(source_file)
            if Path(source_file).exists()
            else "unknown"
        )

        # Pr√©parer m√©tadonn√©es
        if feature_names is None:
            feature_names = list(features.keys())

        metadata = FeatureMetadata(
            feature_set_id=feature_set_id,
            source_file=source_file,
            repository=repository,
            extracted_at=datetime.now(),
            feature_names=feature_names,
            n_features=len(features),
            content_hash=content_hash,
            source_hash=source_hash,
            tags=tags or {},
            extraction_time_ms=extraction_time_ms,
        )

        try:
            # Sauvegarder features
            features_path = self.cache_dir / f"{feature_set_id}.pkl"
            with open(features_path, "wb") as f:
                pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)

            # Sauvegarder m√©tadonn√©es
            metadata_path = self.metadata_dir / f"{feature_set_id}_metadata.json"
            metadata_dict = metadata.dict()
            # Convertir datetime en string pour JSON
            if "extracted_at" in metadata_dict and isinstance(
                metadata_dict["extracted_at"], datetime
            ):
                metadata_dict["extracted_at"] = metadata_dict[
                    "extracted_at"
                ].isoformat()

            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(metadata_dict, f, indent=2, ensure_ascii=False)

            print(f"‚úÖ Features stock√©es: {feature_set_id}")
            print(f"   üìÅ {len(features)} features pour {source_file}")

            return feature_set_id

        except Exception as e:
            print(f"‚ùå Erreur stockage features: {e}")
            raise

    def get_features(
        self,
        source_file: str,
        repository: str,
        check_freshness: bool = True,
        return_metadata: bool = False,
    ) -> dict[str, Any] | None | tuple[dict[str, Any] | None, FeatureMetadata | None]:
        """
        R√©cup√®re des features stock√©es.

        Args:
            source_file: Chemin du fichier source
            repository: Nom du repository
            check_freshness: V√©rifier fra√Æcheur (TTL + hash fichier)
            return_metadata: Retourner aussi les m√©tadonn√©es

        Returns:
            Features (et m√©tadonn√©es si demand√©es) ou None si non trouv√©es/expir√©es
        """
        feature_set_id = self._generate_feature_set_id(source_file, repository)

        # Chemins
        features_path = self.cache_dir / f"{feature_set_id}.pkl"
        metadata_path = self.metadata_dir / f"{feature_set_id}_metadata.json"

        # V√©rifier existence
        if not features_path.exists() or not metadata_path.exists():
            return (None, None) if return_metadata else None

        try:
            # Charger m√©tadonn√©es
            with open(metadata_path, encoding="utf-8") as f:
                metadata_dict = json.load(f)

            # Convertir string datetime en datetime object
            if "extracted_at" in metadata_dict and isinstance(
                metadata_dict["extracted_at"], str
            ):
                metadata_dict["extracted_at"] = datetime.fromisoformat(
                    metadata_dict["extracted_at"]
                )

            metadata = FeatureMetadata(**metadata_dict)

            # V√©rifier fra√Æcheur si demand√©
            if check_freshness and not self._is_fresh(metadata, source_file):
                print(f"üìÖ Features expir√©es pour {source_file}")
                return (None, None) if return_metadata else None

            # Charger features
            with open(features_path, "rb") as f:
                features = pickle.load(f)

            print(f"‚úÖ Features r√©cup√©r√©es du cache: {feature_set_id}")

            if return_metadata:
                return features, metadata
            else:
                return features

        except Exception as e:
            print(f"‚ùå Erreur chargement features: {e}")
            return (None, None) if return_metadata else None

    def list_feature_sets(
        self, repository: str | None = None, include_expired: bool = False
    ) -> list[dict[str, Any]]:
        """
        Liste les ensembles de features disponibles.

        Args:
            repository: Filtrer par repository
            include_expired: Inclure les features expir√©es

        Returns:
            Liste des m√©tadonn√©es des feature sets
        """
        feature_sets = []

        for metadata_file in self.metadata_dir.glob("*_metadata.json"):
            try:
                with open(metadata_file, encoding="utf-8") as f:
                    metadata_dict = json.load(f)

                # Convertir string datetime en datetime object
                if "extracted_at" in metadata_dict and isinstance(
                    metadata_dict["extracted_at"], str
                ):
                    metadata_dict["extracted_at"] = datetime.fromisoformat(
                        metadata_dict["extracted_at"]
                    )

                metadata = FeatureMetadata(**metadata_dict)

                # Filtrer par repository si sp√©cifi√©
                if repository and metadata.repository != repository:
                    continue

                # V√©rifier fra√Æcheur si demand√©
                if not include_expired:
                    if not self._is_fresh(metadata, metadata.source_file):
                        continue

                # Ajouter informations suppl√©mentaires
                info = metadata.dict()
                info["is_fresh"] = self._is_fresh(metadata, metadata.source_file)

                # Taille du cache
                features_path = self.cache_dir / f"{metadata.feature_set_id}.pkl"
                if features_path.exists():
                    info["cache_size_mb"] = round(
                        features_path.stat().st_size / (1024 * 1024), 2
                    )

                feature_sets.append(info)

            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lecture m√©tadonn√©es {metadata_file}: {e}")
                continue

        # Trier par date de cr√©ation
        feature_sets.sort(key=lambda x: x["extracted_at"], reverse=True)
        return feature_sets

    def cleanup_expired(self) -> int:
        """
        Nettoie les features expir√©es.

        Returns:
            Nombre de feature sets supprim√©s
        """
        deleted_count = 0

        for metadata_file in self.metadata_dir.glob("*_metadata.json"):
            try:
                with open(metadata_file, encoding="utf-8") as f:
                    metadata_dict = json.load(f)

                # Convertir string datetime en datetime object
                if "extracted_at" in metadata_dict and isinstance(
                    metadata_dict["extracted_at"], str
                ):
                    metadata_dict["extracted_at"] = datetime.fromisoformat(
                        metadata_dict["extracted_at"]
                    )

                metadata = FeatureMetadata(**metadata_dict)

                # V√©rifier si expir√©
                if not self._is_fresh(metadata, metadata.source_file):
                    # Supprimer fichiers associ√©s
                    feature_set_id = metadata.feature_set_id
                    features_path = self.cache_dir / f"{feature_set_id}.pkl"

                    files_deleted = []
                    for path in [features_path, metadata_file]:
                        if path.exists():
                            path.unlink()
                            files_deleted.append(path.name)

                    if files_deleted:
                        deleted_count += 1
                        print(f"üóëÔ∏è Supprim√© feature set expir√©: {feature_set_id}")

            except Exception as e:
                print(f"‚ö†Ô∏è Erreur nettoyage {metadata_file}: {e}")
                continue

        if deleted_count > 0:
            print(f"‚úÖ Nettoyage termin√©: {deleted_count} feature sets supprim√©s")
        else:
            print("üìù Aucun feature set expir√© √† nettoyer")

        return deleted_count

    def get_feature_statistics(self) -> dict[str, Any]:
        """R√©cup√®re des statistiques sur le feature store."""
        all_sets = self.list_feature_sets(include_expired=True)
        fresh_sets = self.list_feature_sets(include_expired=False)

        repositories = {fs["repository"] for fs in all_sets}
        total_cache_size = sum(fs.get("cache_size_mb", 0) for fs in all_sets)

        # Statistiques par repository
        repo_stats = {}
        for repo in repositories:
            repo_sets = [fs for fs in all_sets if fs["repository"] == repo]
            repo_stats[repo] = {
                "total_feature_sets": len(repo_sets),
                "fresh_feature_sets": len(
                    [fs for fs in repo_sets if fs.get("is_fresh", False)]
                ),
                "cache_size_mb": sum(fs.get("cache_size_mb", 0) for fs in repo_sets),
            }

        return {
            "total_feature_sets": len(all_sets),
            "fresh_feature_sets": len(fresh_sets),
            "expired_feature_sets": len(all_sets) - len(fresh_sets),
            "total_cache_size_mb": total_cache_size,
            "unique_repositories": len(repositories),
            "repository_stats": repo_stats,
            "most_recent_extraction": (
                max(fs["extracted_at"] for fs in all_sets) if all_sets else None
            ),
        }

    def search_features(
        self, query: str, search_in: list[str] = None
    ) -> list[dict[str, Any]]:
        """
        Recherche des feature sets par query.

        Args:
            query: Terme de recherche
            search_in: Champs dans lesquels chercher

        Returns:
            Feature sets correspondants
        """
        if search_in is None:
            search_in = ["source_file", "feature_names", "tags"]
        results = []
        all_sets = self.list_feature_sets(include_expired=True)

        query_lower = query.lower()

        for feature_set in all_sets:
            match_found = False

            # Rechercher dans les champs sp√©cifi√©s
            for field in search_in:
                if (
                    field == "source_file"
                    and query_lower in feature_set["source_file"].lower()
                ):
                    match_found = True
                    break
                elif field == "feature_names":
                    if any(
                        query_lower in fname.lower()
                        for fname in feature_set["feature_names"]
                    ):
                        match_found = True
                        break
                elif field == "tags":
                    tags_text = " ".join(
                        f"{k}:{v}" for k, v in feature_set["tags"].items()
                    )
                    if query_lower in tags_text.lower():
                        match_found = True
                        break

            if match_found:
                results.append(feature_set)

        return results

    def _generate_feature_set_id(self, source_file: str, repository: str) -> str:
        """G√©n√®re un ID unique pour un ensemble de features."""
        # Normaliser les chemins
        normalized_file = str(Path(source_file).resolve())
        content = f"{repository}::{normalized_file}"

        # Hash pour ID court mais unique
        return hashlib.md5(content.encode()).hexdigest()[:16]

    def _calculate_content_hash(self, features: dict[str, Any]) -> str:
        """Calcule le hash du contenu des features."""
        # S√©rialiser de mani√®re d√©terministe
        content_str = str(sorted(features.items()))
        return hashlib.sha256(content_str.encode()).hexdigest()[:16]

    def _calculate_file_hash(self, file_path: str) -> str:
        """Calcule le hash MD5 d'un fichier."""
        try:
            with open(file_path, "rb") as f:
                content = f.read()
            return hashlib.md5(content).hexdigest()
        except Exception:
            return "unknown"

    def _is_fresh(self, metadata: FeatureMetadata, source_file: str) -> bool:
        """
        V√©rifie si des features sont encore fra√Æches.

        Args:
            metadata: M√©tadonn√©es des features
            source_file: Fichier source

        Returns:
            True si les features sont fra√Æches
        """
        now = datetime.now()

        # V√©rifier TTL
        if now - metadata.extracted_at > self.ttl:
            return False

        # V√©rifier si le fichier source a chang√©
        if Path(source_file).exists():
            current_hash = self._calculate_file_hash(source_file)
            if current_hash != metadata.source_hash and current_hash != "unknown":
                return False

        return True


# Instance globale du feature store
feature_store = FeatureStore()
