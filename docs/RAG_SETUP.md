# ðŸš€ Setup Hyperion RAG - Guide d'installation complet

Ce guide configure le RAG 100% local avec Qdrant + Ollama + BGE embeddings.

---

## ðŸ“‹ PrÃ©requis

- âœ… Python 3.10+
- âœ… GPU NVIDIA avec CUDA (RTX 4090 dÃ©tectÃ©)
- âœ… 30 GB RAM minimum
- âœ… 50 GB espace disque libre

---

## 1ï¸âƒ£ Installation Qdrant (Vector Store)

### Option A : Docker (recommandÃ©)

```bash
# Lancer Qdrant
docker run -d \
  --name qdrant \
  -p 6333:6333 \
  -p 6334:6334 \
  -v $(pwd)/qdrant_storage:/qdrant/storage \
  qdrant/qdrant

# VÃ©rifier
curl http://localhost:6333/
```

### Option B : Installation manuelle

```bash
# TÃ©lÃ©charger
wget https://github.com/qdrant/qdrant/releases/download/v1.7.0/qdrant-x86_64-unknown-linux-gnu.tar.gz

# Extraire
tar -xzf qdrant-x86_64-unknown-linux-gnu.tar.gz

# Lancer
./qdrant
```

**Dashboard Qdrant** : http://localhost:6333/dashboard

---

## 2ï¸âƒ£ Installation Ollama (LLM Local)

```bash
# Installation
curl -fsSL https://ollama.com/install.sh | sh

# VÃ©rifier
ollama --version

# Lancer le service
ollama serve
```

### TÃ©lÃ©charger le modÃ¨le Qwen 2.5 32B

```bash
# TÃ©lÃ©charger (~19 GB, peut prendre 10-15 min)
ollama pull qwen2.5:32b

# Tester
ollama run qwen2.5:32b "Bonjour, peux-tu te prÃ©senter ?"
```

**Alternative si 32B trop gros** :
```bash
# Qwen 2.5 14B (plus petit, ~8 GB)
ollama pull qwen2.5:14b

# Ou Llama 3.2 8B (encore plus petit, ~4.7 GB)
ollama pull llama3.2:latest
```

**Configuration** : Editer `.env`
```bash
OLLAMA_MODEL=qwen2.5:32b  # Ou qwen2.5:14b, llama3.2
```

---

## 3ï¸âƒ£ Installation dÃ©pendances Python

```bash
cd /home/kortazo/Documents/Hyperion

# Installer PyTorch avec CUDA (pour GPU)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --break-system-packages

# Installer les dÃ©pendances Hyperion
pip install -r requirements.txt --break-system-packages
```

**VÃ©rifier CUDA** :
```bash
python3 -c "import torch; print(f'CUDA disponible: {torch.cuda.is_available()}')"
# Devrait afficher : CUDA disponible: True
```

---

## 4ï¸âƒ£ Configuration Hyperion

### Mettre Ã  jour `.env`

```bash
# Ajouter Ã  .env
cat >> .env << 'EOF'

# === RAG Configuration ===
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION=hyperion_repos

# Embeddings (GPU)
EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
EMBEDDING_DEVICE=cuda

# LLM Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:32b
LLM_TEMPERATURE=0.1
LLM_TOP_K=5
EOF
```

---

## 5ï¸âƒ£ Ingestion des donnÃ©es

### Script d'ingestion

CrÃ©er `scripts/ingest_rag.py` :

```bash
cd /home/kortazo/Documents/Hyperion
python3 scripts/ingest_rag.py
```

Ce script va :
1. Charger tous les profils YAML
2. DÃ©couper en chunks sÃ©mantiques
3. GÃ©nÃ©rer embeddings avec BGE-large (GPU)
4. Uploader vers Qdrant

**DurÃ©e estimÃ©e** : 1-2 minutes pour 1 repo

---

## 6ï¸âƒ£ Test du RAG

### Test en ligne de commande

```bash
cd /home/kortazo/Documents/Hyperion
python3 scripts/test_rag.py
```

### Test via API

```bash
# Lancer l'API
python3 scripts/run_dashboard.py

# Dans un autre terminal
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Qui est le contributeur principal de requests ?",
    "repo": "requests"
  }'
```

---

## 7ï¸âƒ£ VÃ©rifications

### Qdrant

```bash
# Nombre de points
curl http://localhost:6333/collections/hyperion_repos

# Dashboard
firefox http://localhost:6333/dashboard
```

### Ollama

```bash
# Liste modÃ¨les
ollama list

# Info modÃ¨le
ollama show qwen2.5:32b
```

### GPU

```bash
# VÃ©rifier utilisation GPU
nvidia-smi

# Devrait montrer Python utilisant la VRAM
```

---

## ðŸ› Troubleshooting

### Qdrant ne dÃ©marre pas

```bash
# VÃ©rifier port disponible
ss -ltnp | grep 6333

# Logs Docker
docker logs qdrant
```

### Ollama erreur

```bash
# RedÃ©marrer service
systemctl restart ollama

# VÃ©rifier port
ss -ltnp | grep 11434
```

### CUDA pas dÃ©tectÃ©

```bash
# VÃ©rifier driver NVIDIA
nvidia-smi

# RÃ©installer PyTorch
pip uninstall torch
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

### Embeddings lents

```bash
# VÃ©rifier device utilisÃ©
python3 -c "from sentence_transformers import SentenceTransformer; m = SentenceTransformer('BAAI/bge-large-en-v1.5', device='cuda'); print(m.device)"
```

---

## ðŸ“Š Utilisation mÃ©moire estimÃ©e

| Composant | RAM | VRAM |
|-----------|-----|------|
| Qdrant | ~500 MB | 0 GB |
| BGE-large embeddings | ~1 GB | ~2 GB |
| Qwen 2.5 32B | 0 GB | ~19 GB |
| **Total** | **~2 GB** | **~21 GB** |

**Ta config** : 30 GB RAM, 24 GB VRAM â†’ Largement suffisant ! âœ…

---

## âš¡ Performance attendue

Avec ta RTX 4090 :
- **Embedding** : ~100 chunks/sec
- **LLM inference** : ~30 tokens/sec
- **RÃ©ponse simple** : 1-2 sec
- **RÃ©ponse complexe** : 3-5 sec

---

## ðŸŽ¯ Prochaine Ã©tape

Une fois setup terminÃ© :

```bash
# Lancer le dashboard complet
python3 scripts/run_dashboard.py
```

Dashboard avec chat RAG disponible sur http://localhost:3000 ! ðŸŽ‰

---

## ðŸ’¡ Commandes utiles

```bash
# Status services
systemctl status ollama
docker ps | grep qdrant

# RÃ©indexer un repo
python3 scripts/ingest_rag.py --repo requests --clear

# Test rapide
echo '{"question": "Combien de commits ?", "repo": "requests"}' | \
  http POST localhost:8000/api/chat
```

---

**Installation terminÃ©e ! Le RAG est prÃªt Ã  fonctionner ! ðŸš€**
