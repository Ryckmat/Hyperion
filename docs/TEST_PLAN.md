# üß™ PLAN DE TEST & BENCHMARK HYPERION V2

**Auteur** : Ryckman Matthieu  
**Projet** : Hyperion (projet personnel)  
**Version** : 2.0.0  
**Date** : 23 d√©cembre 2024

---

## üìã TABLE DES MATI√àRES

1. [Objectifs](#objectifs)
2. [Strat√©gie de test](#strat√©gie-de-test)
3. [Tests unitaires](#tests-unitaires)
4. [Tests d'int√©gration](#tests-dint√©gration)
5. [Tests E2E](#tests-e2e)
6. [Benchmarks performance](#benchmarks-performance)
7. [Crit√®res d'acceptation](#crit√®res-dacceptation)
8. [Environnement de test](#environnement-de-test)

---

## 1. OBJECTIFS

### Objectifs g√©n√©raux
- ‚úÖ Valider le bon fonctionnement des 8 modules v2
- ‚úÖ Mesurer les performances (temps r√©ponse, m√©moire)
- ‚úÖ Garantir 90%+ de couverture de code
- ‚úÖ Identifier les goulots d'√©tranglement
- ‚úÖ Valider l'int√©gration avec services existants (Qdrant, Neo4j, Ollama)

### M√©triques cibles

| M√©trique | Cible | Critique |
|----------|-------|----------|
| **Coverage tests** | ‚â• 90% | Oui |
| **Temps r√©ponse RAG** | < 5s p95 | Oui |
| **Temps analyse impact** | < 3s | Oui |
| **Temps indexation code** | < 10s/100 fichiers | Non |
| **M√©moire RAM** | < 2GB | Non |
| **Tests pass** | 100% | Oui |

---

## 2. STRAT√âGIE DE TEST

### Pyramide de tests

```
         /\
        /E2E\         5% - Tests end-to-end
       /------\
      /Int√©gra-\      15% - Tests int√©gration
     /----------\
    /  Unitaires \    80% - Tests unitaires
   /--------------\
```

### Priorit√©s

| Priorit√© | Module | Justification |
|----------|--------|---------------|
| **P0** | Impact Analysis | Feature critique pour v2 |
| **P0** | Code Understanding | Feature critique pour v2 |
| **P1** | Anomaly Detection | S√©curit√©/RGPD important |
| **P1** | RAG (existant) | Validation non-r√©gression |
| **P2** | Onboarding | Nice-to-have |
| **P2** | Autres modules | Stubs, impl√©mentation future |

---

## 3. TESTS UNITAIRES

### 3.1 Module Impact Analysis

#### Test: `test_impact_analyzer.py`

**Sc√©narios** :
```python
def test_analyzer_initialization(temp_repo):
    """V√©rifie initialisation correcte."""
    
def test_analyze_file_simple(temp_repo):
    """Analyse fichier Python basique."""
    
def test_analyze_file_complex(temp_repo):
    """Analyse fichier avec imports multiples."""
    
def test_extract_imports_various_formats(temp_repo):
    """Test imports: from X import Y, import X as Y."""
    
def test_extract_functions_with_decorators(temp_repo):
    """Test extraction fonctions d√©cor√©es."""
    
def test_extract_classes_inheritance(temp_repo):
    """Test extraction classes avec h√©ritage."""
    
def test_build_dependency_graph_cyclic(temp_repo):
    """D√©tection d√©pendances circulaires."""
    
def test_get_impacted_files_depth(temp_repo):
    """Test impact avec profondeur variable."""
```

**Fixtures** :
```python
@pytest.fixture
def temp_repo(tmp_path):
    """Cr√©√© repo test avec structure r√©aliste."""
    
@pytest.fixture
def complex_codebase(tmp_path):
    """Cr√©√© 50+ fichiers interd√©pendants."""
```

#### Test: `test_impact_predictor.py`

**Sc√©narios** :
```python
def test_extract_features_complete():
    """Extraction toutes features ML."""
    
def test_predict_risk_thresholds():
    """Validation seuils LOW/MEDIUM/HIGH/CRITICAL."""
    
def test_risk_score_normalization():
    """Score toujours entre 0 et 1."""
    
def test_predict_risk_edge_cases():
    """Cas limites: 0 deps, 1000+ deps."""
```

#### Test: `test_impact_report.py`

**Sc√©narios** :
```python
def test_create_report_complete():
    """Cr√©ation rapport avec tous champs."""
    
def test_recommendations_by_risk_level():
    """Recommandations adapt√©es au risque."""
    
def test_to_json_valid():
    """Export JSON valide."""
    
def test_to_markdown_formatting():
    """Export Markdown bien format√©."""
    
def test_save_report_permissions(tmp_path):
    """Gestion erreurs √©criture."""
```

### 3.2 Module Understanding

#### Test: `test_understanding_indexer.py`

**Sc√©narios** :
```python
def test_index_file_complete():
    """Indexation fichier complet."""
    
def test_extract_docstrings_all_types():
    """Docstrings: module, classe, fonction, m√©thode."""
    
def test_extract_function_signatures_complex():
    """Signatures: args, kwargs, *args, **kwargs, annotations."""
    
def test_extract_comments_multiline():
    """Commentaires inline et multilignes."""
    
def test_index_repository_large(large_repo):
    """Performance sur gros repo (1000+ fichiers)."""
```

### 3.3 Module Anomaly

#### Test: `test_anomaly_detector.py`

**Sc√©narios** :
```python
def test_detect_high_complexity():
    """D√©tection complexit√© > 15."""
    
def test_detect_long_functions():
    """D√©tection fonctions > 100 lignes."""
    
def test_extract_metrics_accuracy():
    """Pr√©cision m√©triques calcul√©es."""
    
def test_scan_repository_performance(large_repo):
    """Performance scan complet."""
```

#### Test: `test_anomaly_patterns.py`

**Sc√©narios** :
```python
def test_detect_sql_injection():
    """D√©tection SQL concatenation."""
    
def test_detect_hardcoded_secrets():
    """D√©tection passwords/API keys."""
    
def test_detect_command_injection():
    """D√©tection os.system/subprocess."""
    
def test_check_rgpd_pii_data():
    """D√©tection donn√©es PII non chiffr√©es."""
    
def test_generate_report_complete():
    """Rapport avec tous types findings."""
```

### 3.4 Coverage cible

```bash
# Ex√©cution tests unitaires
pytest tests/unit/ -v --cov=hyperion.modules --cov-report=html

# Cible par module
- impact/        : ‚â• 90%
- understanding/ : ‚â• 90%
- anomaly/       : ‚â• 90%
- onboarding/    : ‚â• 80%
- autres/        : ‚â• 70%
```

---

## 4. TESTS D'INT√âGRATION

### 4.1 Workflow Impact Analysis complet

**Fichier** : `tests/integration/test_impact_flow.py`

**Sc√©nario** :
1. Cr√©er repo test avec 20+ fichiers
2. Analyser avec `ImpactAnalyzer`
3. Pr√©dire risque avec `RiskPredictor`
4. Requ√™ter Neo4j avec `GraphTraversal`
5. G√©n√©rer rapport avec `ImpactReport`
6. Valider coh√©rence donn√©es

**Assertions** :
```python
assert len(dependency_graph) >= 20
assert risk_level in [RiskLevel.LOW, RiskLevel.MEDIUM, RiskLevel.HIGH, RiskLevel.CRITICAL]
assert 0.0 <= risk_score <= 1.0
assert len(report.recommendations) > 0
```

### 4.2 Pipeline Ingestion g√©n√©ralis√©

**Fichier** : `tests/integration/test_ingestion_generalized.py`

**Sc√©nario** :
1. Pr√©parer sources test (Git, Docs, Code)
2. Lancer ingestion compl√®te
3. V√©rifier indexation Qdrant
4. V√©rifier graphe Neo4j
5. Tester requ√™tes RAG

**Assertions** :
```python
assert stats['git'] > 0
assert stats['docs'] > 0
assert stats['code'] > 0
# V√©rifier Qdrant
collection_info = qdrant_client.get_collection("hyperion")
assert collection_info.points_count > 0
# V√©rifier Neo4j
result = neo4j_session.run("MATCH (n) RETURN count(n)")
assert result.single()[0] > 0
```

### 4.3 RAG multi-sources

**Fichier** : `tests/integration/test_rag_multi_sources.py`

**Sc√©nario** :
1. Indexer Git + Code + Docs
2. Requ√™te cross-sources: "O√π est le calcul de prix et qui l'a modifi√©?"
3. Valider r√©ponse combine Git + Code

### 4.4 Neo4j + Impact Analysis

**Fichier** : `tests/integration/test_neo4j_impact.py`

**Sc√©nario** :
1. Ing√©rer code dans Neo4j
2. Cr√©er relations DEPENDS_ON
3. Requ√™ter via GraphTraversal
4. Comparer r√©sultats AST vs Neo4j

---

## 5. TESTS E2E

### 5.1 Workflow utilisateur complet

**Fichier** : `tests/e2e/test_complete_workflow_v2.py`

**Sc√©nario utilisateur** :
```
GIVEN un nouveau repository
WHEN j'ex√©cute le pipeline complet
THEN je peux:
  1. Analyser l'impact d'une modification
  2. Trouver o√π une feature est impl√©ment√©e
  3. D√©tecter anomalies/patterns dangereux
  4. G√©n√©rer rapport complet
```

**√âtapes** :
```python
def test_complete_v2_workflow(sample_repo):
    # 1. Ingestion
    ingestion = GeneralizedIngestion()
    stats = ingestion.run(repo_path=sample_repo)
    
    # 2. Impact analysis
    analyzer = ImpactAnalyzer(sample_repo)
    graph = analyzer.build_dependency_graph()
    predictor = RiskPredictor()
    risk = predictor.predict_risk("core/api.py", graph)
    
    # 3. Code understanding
    engine = UnderstandingQueryEngine(sample_repo)
    response = engine.query("O√π est l'authentification ?")
    
    # 4. Anomaly detection
    detector = AnomalyDetector(sample_repo)
    anomalies = detector.scan_repository()
    
    # 5. Rapports
    assert risk is not None
    assert response['confidence'] > 0.5
    assert len(anomalies) >= 0
```

### 5.2 Test CLI

**Fichier** : `tests/e2e/bash/test_cli_v2.sh`

```bash
#!/bin/bash
# Test CLI Hyperion v2

# Test impact analysis
hyperion impact analyze --file src/api.py --repo /path/to/repo
# Expected: rapport JSON g√©n√©r√©

# Test understanding
hyperion understand query "O√π est le paiement ?" --repo /path/to/repo
# Expected: liste fichiers + score

# Test ingestion
python scripts/maintenance/ingest_generalized.py --repo /path/to/repo
# Expected: stats affich√©es
```

---

## 6. BENCHMARKS PERFORMANCE

### 6.1 Impact Analysis

**Script** : `tests/benchmarks/bench_impact.py`

```python
import time
from hyperion.modules.impact import ImpactAnalyzer, RiskPredictor

def bench_analyze_small_repo():
    """Benchmark sur petit repo (10 fichiers)."""
    
def bench_analyze_medium_repo():
    """Benchmark sur repo moyen (100 fichiers)."""
    
def bench_analyze_large_repo():
    """Benchmark sur gros repo (1000 fichiers)."""
    
def bench_predict_risk():
    """Benchmark pr√©diction risque."""
```

**M√©triques** :
```
Repo 10 fichiers   : < 1s
Repo 100 fichiers  : < 3s  ‚úÖ CRITIQUE
Repo 1000 fichiers : < 30s
```

### 6.2 Code Understanding

**Script** : `tests/benchmarks/bench_understanding.py`

```python
def bench_index_repository():
    """Benchmark indexation compl√®te."""
    
def bench_query_engine():
    """Benchmark requ√™te RAG."""
```

**M√©triques** :
```
Indexation 100 fichiers : < 10s ‚úÖ CRITIQUE
Query response          : < 5s  ‚úÖ CRITIQUE
```

### 6.3 RAG Performance

**Script** : `tests/benchmarks/bench_rag.py`

```python
def bench_rag_query_simple():
    """Requ√™te simple (1 chunk)."""
    
def bench_rag_query_complex():
    """Requ√™te complexe (10+ chunks)."""
    
def bench_rag_with_filters():
    """Requ√™te avec filtres repository."""
```

**M√©triques** :
```
Query simple  : < 1s
Query complex : < 5s  ‚úÖ CRITIQUE
P95 latency   : < 5s  ‚úÖ CRITIQUE
```

### 6.4 Ingestion

**Script** : `tests/benchmarks/bench_ingestion.py`

```python
def bench_ingest_git():
    """Benchmark ingestion Git."""
    
def bench_ingest_code():
    """Benchmark ingestion code analysis."""
    
def bench_ingest_docs():
    """Benchmark ingestion documentation."""
```

**M√©triques** :
```
Git 1000 commits   : < 30s
Code 100 fichiers  : < 15s
Docs 50 Markdown   : < 5s
```

---

## 7. CRIT√àRES D'ACCEPTATION

### 7.1 Tests

| Crit√®re | Cible | Status |
|---------|-------|--------|
| Tests unitaires pass | 100% | ‚è≥ |
| Tests int√©gration pass | 100% | ‚è≥ |
| Tests E2E pass | 100% | ‚è≥ |
| Coverage globale | ‚â• 90% | ‚è≥ |
| Coverage impact/ | ‚â• 95% | ‚è≥ |
| Coverage understanding/ | ‚â• 95% | ‚è≥ |

### 7.2 Performance

| Crit√®re | Cible | Status |
|---------|-------|--------|
| RAG query p95 | < 5s | ‚è≥ |
| Impact analysis | < 3s | ‚è≥ |
| Indexation 100 fichiers | < 10s | ‚è≥ |
| M√©moire RAM max | < 2GB | ‚è≥ |

### 7.3 Qualit√© code

| Crit√®re | Cible | Status |
|---------|-------|--------|
| Ruff/Black pass | 100% | ‚è≥ |
| Type hints | 100% | ‚úÖ |
| Docstrings | 100% | ‚úÖ |
| Tests par module | ‚â• 1 | ‚úÖ |

---

## 8. ENVIRONNEMENT DE TEST

### 8.1 Configuration

```yaml
# pytest.ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short
    --cov=hyperion.modules
    --cov-report=html
    --cov-report=term-missing
    --durations=10
markers =
    unit: Tests unitaires
    integration: Tests int√©gration
    e2e: Tests end-to-end
    slow: Tests lents (> 5s)
    benchmark: Benchmarks performance
```

### 8.2 Services requis

```yaml
# docker-compose.test.yml
version: '3.8'
services:
  qdrant-test:
    image: qdrant/qdrant:v1.7.4
    ports:
      - "6334:6333"
    
  neo4j-test:
    image: neo4j:5.15.0
    environment:
      NEO4J_AUTH: neo4j/testpassword
    ports:
      - "7475:7474"
      - "7688:7687"
```

### 8.3 Fixtures globales

```python
# tests/conftest.py
import pytest
from pathlib import Path

@pytest.fixture(scope="session")
def test_repo_root():
    return Path(__file__).parent.parent

@pytest.fixture(scope="session")
def qdrant_test_client():
    from qdrant_client import QdrantClient
    return QdrantClient(host="localhost", port=6334)

@pytest.fixture(scope="session")
def neo4j_test_driver():
    from neo4j import GraphDatabase
    return GraphDatabase.driver(
        "bolt://localhost:7688",
        auth=("neo4j", "testpassword")
    )

@pytest.fixture
def sample_repo(tmp_path):
    """Cr√©√© un repository d'exemple pour tests."""
    # TODO: Impl√©menter cr√©ation repo r√©aliste
    pass
```

---

## 9. PLAN D'EX√âCUTION

### Phase 1 : Tests unitaires (2 jours)

```bash
# Jour 1 : Modules prioritaires
pytest tests/unit/test_impact_*.py -v
pytest tests/unit/test_understanding_*.py -v

# Jour 2 : Autres modules
pytest tests/unit/test_anomaly_*.py -v
pytest tests/unit/test_onboarding_*.py -v
```

### Phase 2 : Tests int√©gration (1 jour)

```bash
# Lancer services test
docker-compose -f docker-compose.test.yml up -d

# Tests int√©gration
pytest tests/integration/ -v --maxfail=1

# Cleanup
docker-compose -f docker-compose.test.yml down
```

### Phase 3 : Tests E2E (1 jour)

```bash
# Workflow complet
pytest tests/e2e/ -v -s

# CLI tests
./tests/e2e/bash/test_cli_v2.sh
```

### Phase 4 : Benchmarks (1 jour)

```bash
# Performance tests
pytest tests/benchmarks/ -v --benchmark-only

# G√©n√©ration rapport
pytest-benchmark compare --csv=benchmarks_v2.csv
```

### Phase 5 : Rapport final (0.5 jour)

```bash
# Coverage complet
pytest --cov=hyperion --cov-report=html

# Rapport qualit√©
ruff check src/ tests/
mypy src/

# G√©n√©ration rapport final
python scripts/generate_test_report.py
```

---

## 10. COMMANDES RAPIDES

```bash
# Tests complets
make test

# Tests unitaires seulement
make test-unit

# Tests avec coverage
make test-coverage

# Tests lents exclus
pytest -m "not slow"

# Tests d'un module sp√©cifique
pytest tests/unit/test_impact_analyzer.py -v

# Benchmarks
make benchmark

# Linting
make lint

# Format code
make format

# Tout nettoyer
make clean
```

---

## 11. LIVRABLES

### Rapports attendus

1. **coverage_report.html** : Coverage d√©taill√© par module
2. **benchmark_results.json** : R√©sultats benchmarks
3. **test_summary.md** : Synth√®se pass/fail
4. **performance_analysis.pdf** : Analyse performance

### Dashboards

- **Pytest HTML Report** : D√©tails tests
- **Coverage.py Dashboard** : Visualisation coverage
- **Benchmark Dashboard** : Graphiques performance

---

## üìä DASHBOARD SUIVI

| Module | Tests Unit | Tests Integ | Coverage | Perf |
|--------|------------|-------------|----------|------|
| impact | ‚è≥ 0/8 | ‚è≥ 0/2 | ‚è≥ 0% | ‚è≥ |
| understanding | ‚è≥ 0/3 | ‚è≥ 0/1 | ‚è≥ 0% | ‚è≥ |
| anomaly | ‚è≥ 0/3 | ‚è≥ 0/1 | ‚è≥ 0% | ‚è≥ |
| onboarding | ‚è≥ 0/1 | ‚è≥ 0/0 | ‚è≥ 0% | ‚è≥ |
| **TOTAL** | **‚è≥ 0/15** | **‚è≥ 0/4** | **‚è≥ 0%** | **‚è≥** |

---

**Pr√™t √† d√©marrer les tests !** üß™üöÄ
