#!/usr/bin/env python3
"""
Test simple de validation du syst√®me de qualit√© v2.8

Ce script teste le syst√®me de validation qualit√© sans n√©cessiter
de services externes (Qdrant, Neo4j, etc.)
"""

import os
import sys
from pathlib import Path

# Ajouter src au path
sys.path.insert(0, str(Path(__file__).parent / "src"))

# Configurer variables d'environnement pour test
os.environ["ENABLE_RESPONSE_VALIDATION"] = "true"
os.environ["VALIDATION_MODE"] = "flag"
os.environ["CONFIDENCE_THRESHOLD"] = "0.7"
os.environ["AUTO_REJECT_THRESHOLD"] = "0.3"


def test_hallucination_detector():
    """Test d√©tecteur d'hallucinations"""
    print("üîç Test HallucinationDetector...")

    # Mock embedding model
    class MockEmbeddingModel:
        def encode(self, texts):
            return [
                [0.1, 0.2, 0.3, 0.4] for _ in range(len(texts) if isinstance(texts, list) else 1)
            ]

    from hyperion.modules.rag.quality.hallucination_detector import HallucinationDetector

    detector = HallucinationDetector(MockEmbeddingModel())

    # Test hallucination √©vidente
    result = detector.detect(
        answer="Selon mes sources, je pense que probablement le projet a 999 contributeurs.",
        context_chunks=["Le projet a 5 contributeurs principaux."],
        question="Combien de contributeurs ?",
    )

    assert result["is_hallucination"]
    assert result["confidence"] < 0.6
    assert len(result["flags"].suspicious_patterns) > 0

    print(
        f"  ‚úÖ Hallucination d√©tect√©e: confiance={result['confidence']:.2f}, s√©v√©rit√©={result['severity']}"
    )

    # Test bonne r√©ponse
    result2 = detector.detect(
        answer="Le projet a 5 contributeurs principaux.",
        context_chunks=["Le projet a 5 contributeurs principaux avec plus de 100 commits."],
        question="Combien de contributeurs ?",
    )

    assert not result2["is_hallucination"]
    assert result2["confidence"] > 0.6

    print(
        f"  ‚úÖ Bonne r√©ponse valid√©e: confiance={result2['confidence']:.2f}, s√©v√©rit√©={result2['severity']}"
    )


def test_confidence_scorer():
    """Test syst√®me de scoring"""
    print("üìä Test ConfidenceScorer...")

    from hyperion.modules.rag.quality.confidence_scorer import ConfidenceScorer

    scorer = ConfidenceScorer()

    # Test excellente qualit√©
    good_hallucination = {"confidence": 0.9, "is_hallucination": False, "severity": "LOW"}

    result = scorer.compute_confidence(
        hallucination_result=good_hallucination,
        source_scores=[0.92, 0.88],
        question="Quel est le langage principal ?",
        answer="Le langage principal est Python selon l'analyse.",
    )

    assert result["final_confidence"] > 0.8
    assert result["quality_grade"] in ["EXCELLENT", "GOOD"]
    assert result["action"] == "accept"

    print(
        f"  ‚úÖ Excellente qualit√©: confiance={result['final_confidence']:.2f}, grade={result['quality_grade']}"
    )

    # Test faible qualit√©
    bad_hallucination = {"confidence": 0.3, "is_hallucination": True, "severity": "HIGH"}

    result2 = scorer.compute_confidence(
        hallucination_result=bad_hallucination,
        source_scores=[0.4],
        question="Combien de fichiers ?",
        answer="Pas s√ªr",
    )

    assert result2["final_confidence"] < 0.6
    assert result2["action"] in ["flag", "reject"]

    print(
        f"  ‚úÖ Faible qualit√© d√©tect√©e: confiance={result2['final_confidence']:.2f}, grade={result2['quality_grade']}"
    )


def test_response_validator():
    """Test orchestrateur de validation"""
    print("üéØ Test ResponseValidator...")

    # Mock embedding model
    class MockEmbeddingModel:
        def encode(self, texts):
            return [
                [0.1, 0.2, 0.3, 0.4] for _ in range(len(texts) if isinstance(texts, list) else 1)
            ]

    from hyperion.modules.rag.quality.response_validator import ResponseValidator

    validator = ResponseValidator(MockEmbeddingModel())

    # Test validation compl√®te
    result = validator.validate_response(
        answer="Le repository contient 5 contributeurs principaux selon l'analyse Git.",
        question="Combien de contributeurs ?",
        context_chunks=["Analyse Git: 5 contributeurs avec plus de 100 commits."],
        source_scores=[0.9],
        processing_time=1.5,
    )

    assert "action" in result
    assert "confidence" in result
    assert "hallucination_analysis" in result
    assert "confidence_breakdown" in result

    print(
        f"  ‚úÖ Validation compl√®te: action={result['action']}, confiance={result['confidence']:.2f}"
    )
    print(f"  ‚úÖ Hallucination: {result['hallucination_analysis']['severity']}")
    print(f"  ‚úÖ Recommandations: {len(result['recommendations'])} suggestions")


def test_quality_metrics():
    """Test syst√®me de m√©triques"""
    print("üìà Test QualityMetricsTracker...")

    import tempfile

    from hyperion.modules.rag.monitoring.quality_metrics import QualityMetricsTracker

    # Base temporaire
    with tempfile.NamedTemporaryFile(suffix=".db", delete=False) as tmp:
        temp_db = tmp.name

    try:
        tracker = QualityMetricsTracker(db_path=temp_db)

        # Mock validation result
        validation_result = {
            "confidence": 0.85,
            "quality_grade": "GOOD",
            "action": "accept",
            "should_flag": False,
            "hallucination_analysis": {
                "is_hallucination": False,
                "severity": "LOW",
                "semantic_consistency": 0.9,
            },
            "confidence_factors": {"primary_weakness": "none"},
            "validation_metadata": {
                "validation_time": 0.15,
                "num_sources": 2,
                "avg_source_score": 0.8,
                "answer_length": 50,
                "question_length": 20,
                "validator_version": "2.8.0",
            },
        }

        # Track r√©ponse
        tracker.track_response(
            validation_result=validation_result,
            processing_time=1.2,
            question="Test question",
            repo="test-repo",
        )

        # V√©rifier m√©triques
        summary = tracker.get_metrics_summary(hours=1)
        assert summary["total_responses"] == 1
        assert summary["acceptance_rate"] == 100.0

        print(f"  ‚úÖ M√©triques track√©es: {summary['total_responses']} r√©ponses")
        print(f"  ‚úÖ Taux acceptation: {summary['acceptance_rate']}%")
        print(f"  ‚úÖ Confiance moyenne: {summary['avg_confidence']:.2f}")

    finally:
        # Cleanup
        os.unlink(temp_db)


def main():
    """Fonction principale de test"""
    print("üöÄ Test syst√®me de validation qualit√© v2.8")
    print("=" * 50)

    try:
        test_hallucination_detector()
        test_confidence_scorer()
        test_response_validator()
        test_quality_metrics()

        print("\n" + "=" * 50)
        print("‚úÖ TOUS LES TESTS PASS√âS - Syst√®me qualit√© v2.8 op√©rationnel !")
        print("\nüéØ Pr√™t pour int√©gration dans Hyperion")
        print("üìä Fonctionnalit√©s valid√©es :")
        print("   - D√©tection d'hallucinations multi-niveaux")
        print("   - Scoring de confiance global")
        print("   - Validation orchestr√©e compl√®te")
        print("   - Monitoring et m√©triques qualit√©")

        return True

    except Exception as e:
        print(f"\n‚ùå ERREUR DANS LES TESTS: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
