"""
Test d'int√©gration du syst√®me de validation qualit√© v2.8 avec Hyperion

Ce test valide que le syst√®me de qualit√© fonctionne correctement
dans l'environnement complet de Hyperion (API, RAG, base de donn√©es).
"""

import os
import time
from pathlib import Path

import pytest
import requests


class TestQualitySystemIntegration:
    """Tests d'int√©gration syst√®me complet"""

    @classmethod
    def setup_class(cls):
        """Setup une fois pour toute la classe de tests"""
        cls.api_base_url = "http://localhost:8000"
        cls.test_questions = [
            "Combien de fichiers Python dans le repository ?",
            "Qui est le contributeur principal ?",
            "Quelle est l'architecture du projet ?",
            "Combien de lignes de code ?",
        ]

    def test_api_health_includes_quality_monitoring(self):
        """Test que l'API health check inclut le monitoring qualit√©"""
        try:
            response = requests.get(f"{self.api_base_url}/api/health", timeout=10)
            assert response.status_code == 200

            health = response.json()
            assert "status" in health
            assert health["status"] in ["healthy", "warning"]

            # V√©rifier endpoint racine inclut features qualit√©
            root_response = requests.get(f"{self.api_base_url}/", timeout=5)
            assert root_response.status_code == 200

            root_data = root_response.json()
            assert "quality_monitoring" in root_data.get("features", [])
            assert "quality_metrics" in root_data.get("endpoints", {})

        except requests.exceptions.RequestException:
            pytest.skip("API non accessible - d√©marrer avec script master")

    def test_chat_endpoint_with_quality_validation(self):
        """Test endpoint chat avec validation qualit√© activ√©e"""
        try:
            # Test chat normal
            chat_payload = {"question": "Combien de fichiers Python dans Hyperion ?", "repo": None}

            response = requests.post(f"{self.api_base_url}/api/chat", json=chat_payload, timeout=15)

            assert response.status_code == 200
            result = response.json()

            # V√©rifier structure r√©ponse de base
            assert "answer" in result
            assert "sources" in result
            assert "processing_time" in result

            # V√©rifier pr√©sence donn√©es qualit√© si validation activ√©e
            if "quality" in result:
                quality = result["quality"]
                assert "confidence" in quality
                assert "grade" in quality
                assert "action" in quality
                assert "hallucination_detected" in quality
                assert quality["grade"] in ["EXCELLENT", "GOOD", "FAIR", "POOR", "UNACCEPTABLE"]
                assert quality["action"] in ["accept", "flag", "reject"]
                assert isinstance(quality["confidence"], (int, float))
                assert 0.0 <= quality["confidence"] <= 1.0

                print(
                    f"‚úÖ Validation qualit√©: {quality['grade']} (confidence: {quality['confidence']})"
                )

            else:
                print("‚ÑπÔ∏è Validation qualit√© d√©sactiv√©e")

        except requests.exceptions.RequestException:
            pytest.skip("API non accessible - d√©marrer avec script master")

    def test_quality_metrics_endpoints(self):
        """Test endpoints de m√©triques qualit√©"""
        try:
            # Test endpoint m√©triques
            metrics_response = requests.get(
                f"{self.api_base_url}/api/quality/metrics?hours=1", timeout=10
            )

            if metrics_response.status_code == 200:
                metrics = metrics_response.json()

                # V√©rifier structure m√©triques
                expected_fields = [
                    "total_responses",
                    "avg_confidence",
                    "acceptance_rate",
                    "flag_rate",
                    "rejection_rate",
                    "hallucination_rate",
                ]

                for field in expected_fields:
                    assert field in metrics, f"Champ manquant: {field}"

                # V√©rifier types et ranges
                assert isinstance(metrics["total_responses"], int)
                assert 0 <= metrics["acceptance_rate"] <= 100
                assert 0 <= metrics["avg_confidence"] <= 1.0

                print(
                    f"‚úÖ M√©triques qualit√©: {metrics['total_responses']} r√©ponses, {metrics['avg_confidence']:.2f} confidence"
                )

            elif metrics_response.status_code == 503:
                print("‚ÑπÔ∏è Monitoring qualit√© non disponible (service non d√©marr√©)")
            else:
                pytest.fail(f"Erreur endpoint m√©triques: {metrics_response.status_code}")

            # Test endpoint trends
            trends_response = requests.get(
                f"{self.api_base_url}/api/quality/trends?days=1", timeout=10
            )

            if trends_response.status_code == 200:
                trends_data = trends_response.json()
                assert "trends" in trends_data
                assert "period_days" in trends_data
                assert trends_data["period_days"] == 1

            # Test endpoint alertes
            alerts_response = requests.get(f"{self.api_base_url}/api/quality/alerts", timeout=10)

            if alerts_response.status_code == 200:
                alerts_data = alerts_response.json()
                assert "alerts" in alerts_data
                assert "total" in alerts_data
                assert isinstance(alerts_data["alerts"], list)

        except requests.exceptions.RequestException:
            pytest.skip("API non accessible - d√©marrer avec script master")

    def test_multiple_chat_requests_quality_tracking(self):
        """Test tracking qualit√© sur plusieurs requ√™tes chat"""
        try:
            results = []

            # Faire plusieurs requ√™tes avec diff√©rents types de questions
            test_cases = [
                {
                    "question": "Combien de contributeurs ?",
                    "expected_grade": ["GOOD", "FAIR", "EXCELLENT"],
                },
                {"question": "Python files count", "expected_grade": ["GOOD", "FAIR", "EXCELLENT"]},
                {
                    "question": "Quelle est l'architecture ?",
                    "expected_grade": ["GOOD", "FAIR", "POOR"],
                },
                {
                    "question": "Blabla random question",
                    "expected_grade": ["POOR", "FAIR"],
                },  # Question moins pertinente
            ]

            for _i, test_case in enumerate(test_cases):
                chat_payload = {"question": test_case["question"], "repo": None}

                response = requests.post(
                    f"{self.api_base_url}/api/chat", json=chat_payload, timeout=15
                )

                assert response.status_code == 200
                result = response.json()
                results.append(result)

                # Attendre un peu entre requ√™tes
                time.sleep(0.5)

            # V√©rifier que la qualit√© varie selon les questions
            quality_grades = []
            confidence_scores = []

            for result in results:
                if "quality" in result:
                    quality = result["quality"]
                    quality_grades.append(quality["grade"])
                    confidence_scores.append(quality["confidence"])

            if quality_grades:
                print(f"‚úÖ Grades qualit√© obtenus: {quality_grades}")
                print(f"‚úÖ Scores confiance: {[f'{s:.2f}' for s in confidence_scores]}")

                # V√©rifier variation des scores (syst√®me adaptatif)
                assert len(set(quality_grades)) >= 1, "Syst√®me devrait produire des grades vari√©s"

            # V√©rifier que les m√©triques sont mises √† jour
            time.sleep(2)  # Attendre processing plus long

            metrics_response = requests.get(
                f"{self.api_base_url}/api/quality/metrics?hours=1", timeout=10
            )

            if metrics_response.status_code == 200:
                updated_metrics = metrics_response.json()
                # Note: En mode test/int√©gration, les m√©triques peuvent ne pas √™tre persist√©es
                # On v√©rifie juste que l'endpoint r√©pond correctement
                total_responses = updated_metrics.get("total_responses", 0)
                print(f"‚úÖ M√©triques endpoint accessible: {total_responses} r√©ponses track√©es")
                # Test plus flexible pour l'int√©gration
                assert total_responses >= 0, "L'endpoint m√©triques doit retourner un nombre >= 0"

        except requests.exceptions.RequestException:
            pytest.skip("API non accessible - d√©marrer avec script master")

    def test_quality_validation_with_rejection_mode(self):
        """Test validation en mode rejection"""
        try:
            # Test avec une question volontairement probl√©matique
            problematic_question = "Combien de licornes magiques dans le code source ?"

            chat_payload = {"question": problematic_question, "repo": None}

            response = requests.post(f"{self.api_base_url}/api/chat", json=chat_payload, timeout=15)

            assert response.status_code == 200
            result = response.json()

            if "quality" in result:
                quality = result["quality"]

                # Questions absurdes devraient √™tre flagg√©es ou rejet√©es
                if quality["action"] in ["flag", "reject"]:
                    print(f"‚úÖ Question probl√©matique correctement {quality['action']}√©e")
                    print(f"   Confidence: {quality['confidence']:.2f}, Grade: {quality['grade']}")

                # Si r√©ponse modifi√©e, v√©rifier
                if quality.get("answer_modified", False):
                    print("‚úÖ R√©ponse automatiquement modifi√©e par validation")
                    assert (
                        "ne peux pas r√©pondre" in result["answer"].lower()
                        or "reformuler" in result["answer"].lower()
                    )

        except requests.exceptions.RequestException:
            pytest.skip("API non accessible - d√©marrer avec script master")

    def test_quality_database_persistence(self):
        """Test persistance des donn√©es qualit√©"""
        try:
            # Faire une requ√™te
            chat_payload = {"question": "Test persistence", "repo": None}

            response = requests.post(f"{self.api_base_url}/api/chat", json=chat_payload, timeout=10)

            assert response.status_code == 200

            # Attendre un peu pour persistance
            time.sleep(1)

            # V√©rifier stats base de donn√©es
            stats_response = requests.get(f"{self.api_base_url}/api/quality/stats", timeout=10)

            if stats_response.status_code == 200:
                stats = stats_response.json()
                assert "total_records" in stats
                assert "database_size_bytes" in stats
                assert stats["total_records"] >= 1
                assert stats["database_size_bytes"] > 0

                print(
                    f"‚úÖ Base donn√©es qualit√©: {stats['total_records']} records, {stats['database_size_bytes']} bytes"
                )

        except requests.exceptions.RequestException:
            pytest.skip("API non accessible - d√©marrer avec script master")


class TestMasterScriptIntegration:
    """Tests d'int√©gration avec le script master"""

    @pytest.fixture
    def hyperion_project_root(self):
        """Chemin vers la racine du projet Hyperion"""
        current_dir = Path(__file__).parent
        # Remonter jusqu'√† trouver le script master
        while current_dir.parent != current_dir:
            master_script = current_dir / "scripts" / "deploy" / "hyperion_master.sh"
            if master_script.exists():
                return current_dir
            current_dir = current_dir.parent

        pytest.skip("Script master non trouv√©")

    def test_master_script_accessible(self, hyperion_project_root):
        """Test que le script master est accessible"""
        master_script = hyperion_project_root / "scripts" / "deploy" / "hyperion_master.sh"
        assert master_script.exists(), "Script master non trouv√©"
        assert os.access(master_script, os.X_OK), "Script master non ex√©cutable"

    def test_quality_validation_environment_variables(self):
        """Test que les variables d'environnement qualit√© sont correctes"""
        env_file = Path(".env")

        if env_file.exists():
            env_content = env_file.read_text()

            # V√©rifier pr√©sence variables qualit√© v2.8
            required_vars = [
                "ENABLE_RESPONSE_VALIDATION",
                "VALIDATION_MODE",
                "CONFIDENCE_THRESHOLD",
                "AUTO_REJECT_THRESHOLD",
            ]

            for var in required_vars:
                assert var in env_content, f"Variable d'environnement manquante: {var}"

            print("‚úÖ Variables d'environnement qualit√© v2.8 pr√©sentes")

        else:
            pytest.skip("Fichier .env non trouv√©")

    def test_quality_modules_importable(self):
        """Test que les modules qualit√© sont correctement importables"""
        try:
            # Test imports modules qualit√©
            from src.hyperion.modules.rag.monitoring.quality_metrics import (
                QualityMetricsTracker,  # noqa: F401
            )
            from src.hyperion.modules.rag.quality.confidence_scorer import (
                ConfidenceScorer,  # noqa: F401
            )
            from src.hyperion.modules.rag.quality.hallucination_detector import (
                HallucinationDetector,  # noqa: F401
            )
            from src.hyperion.modules.rag.quality.response_validator import (
                ResponseValidator,  # noqa: F401
            )

            print("‚úÖ Tous les modules qualit√© importables")

        except ImportError as e:
            pytest.fail(f"Erreur import module qualit√©: {e}")


def run_integration_tests():
    """Fonction utilitaire pour lancer les tests d'int√©gration"""
    print("üöÄ D√©marrage tests d'int√©gration syst√®me qualit√© v2.8")
    print("=" * 60)

    # Check API accessibility first
    try:
        response = requests.get("http://localhost:8000/api/health", timeout=5)
        if response.status_code == 200:
            print("‚úÖ API Hyperion accessible")
        else:
            print("‚ö†Ô∏è API Hyperion r√©pond mais avec erreur")
    except Exception:
        print("‚ùå API Hyperion non accessible")
        print("   D√©marrez d'abord: ./scripts/deploy/hyperion_master.sh")
        return False

    # Run tests
    exit_code = pytest.main([__file__, "-v", "--tb=short", "--color=yes"])

    return exit_code == 0


if __name__ == "__main__":
    success = run_integration_tests()
    exit(0 if success else 1)
